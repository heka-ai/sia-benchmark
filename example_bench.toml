bench_id = "dummy-benchmark"
provider = "aws"
inference_engine = "vllm"
api_key = "dummy-api-key"

[aws]
region = "us-east-1"
gpu_ami = "ami-09cba9350fc25f2a5"
cpu_ami = "ami-0cd317320985b1898"

# the instance type is the ec2 instance type of the
# instance that will be used to run the benchmark
gpu_instance_type = "g5.xlarge"
cpu_instance_type = "t3.micro"

# login to aws using a profile
# you can create a profile using the aws cli
# aws configure --profile stratumn-staging
profile_name = "aws-profile-name"

# or you can use access and secret keys
# access_key = ""
# secret_key = ""

[benchmark]
token = ""
task = "auto"
dataset_name = "dummy-dataset-name"
dataset_path = "dummy-dataset-path"
hf_revision = "dummy-hf-revision"
hf_split = "dummy-hf-split"
num_prompts = 500
seed = 42
save_result = true
result_filename = "metrics.json"

[instance]
health_check = "/health"

[vllm]
model = "meta-llama/Llama-3.2-3B-Instruct"
seed = 42
dtype = "half"
kv-cache-dtype = "auto"
max-model-len = "4096"
max-num-batched-tokens = ""
max-num-seqs = 1024
tokenizer-mode = "auto"
enable-prefix-caching = ""
quantization = ""
enforce-eager = ""
enable-chunked-prefill = ""
pipeline-parallel-size = ""
tensor-parallel-size = ""
cpu-offload-gb = ""
gpu-memory-utilization = ""
device = ""
task = ""
tokenizer = ""
served-model-name = ""
skip-tokenizer-init = ""
revision = ""
code-revision = ""
tokenizer-revision = ""
trust-remote-code = ""
allowed-local-media-path = ""
download-dir = ""
load-format = ""
config-format = ""
guided-decoding-backend = ""
logits-processor-pattern = ""
model-impl = ""
distributed-executor-backend = ""
max-parallel-loading-workers = ""
ray-workers-use-nsight = ""
block-size = ""
disable-sliding-window = ""
num-lookahead-slots = ""
swap-space = ""
num-gpu-blocks-override = ""
max-logprobs = ""
disable-log-stats = ""
rope-scaling = ""
rope-theta = ""
hf-overrides = ""
max-seq-len-to-capture = ""
disable-custom-all-reduce = ""
tokenizer-pool-size = ""
tokenizer-pool-type = ""
tokenizer-pool-extra-config = ""
limit-mm-per-prompt = ""
mm-processor-kwargs = ""
disable-mm-preprocessor-cache = ""
enable-lora = ""
enable-lora-bias = ""
max-loras = ""
max-lora-rank = ""
lora-extra-vocab-size = ""
lora-dtype = ""
long-lora-scaling-factors = ""
max-cpu-loras = ""
fully-sharded-loras = ""
enable-prompt-adapter = ""
max-prompt-adapters = ""
max-prompt-adapter-token = ""
num-scheduler-steps = ""
multi-step-stream-outputs = ""
scheduler-delay-factor = ""
speculative-model = ""
speculative-model-quantization = ""
num-speculative-tokens = ""
speculative-disable-mqa-scorer = ""
speculative-draft-tensor-parallel-size = ""
speculative-max-model-len = ""
speculative-disable-by-batch-size = ""
ngram-prompt-lookup-max = ""
ngram-prompt-lookup-min = ""
spec-decoding-acceptance-method = ""
typical-acceptance-sampler-posterior-threshold = ""
typical-acceptance-sampler-posterior-alpha = ""
disable-logprobs-during-spec-decoding = ""
model-loader-extra-config = ""
preemption-mode = ""
qlora-adapter-name-or-path = ""
otlp-traces-endpoint = ""
collect-detailed-traces = ""
disable-async-output-proc = ""
scheduling-policy = ""
override-neuron-config = ""
override-pooler-config = ""
compilation-config = ""
kv-transfer-config = ""
worker-cls = ""
generation-config = ""
override-generation-config = ""
enable-sleep-mode = ""
calculate-kv-scales = ""
additional-config = ""
