bench_id = "1234"
provider = "scaleway"
inference_engine = "vllm"
api_key = "api_key_test"

[scaleway]
cpu_instance_type = "scaleway_cpu_instance_type_test"
gpu_instance_type = "scaleway_gpu_instance_type_test"

organization_id = "98e7c046-a065-4225-8a5b-531e42cdb457"
project_id = "9cfe8426-a7a5-4991-a573-7e0090fe1bdd"
region = "fr-par"

profile_name = "default"

access_key = ""
secret_key = ""

[benchmark]
token = "benchmark_token_test"
backend = "openai"
dataset_name = "benchmark_dataset_name_test"
dataset_path = "benchmark_dataset_path_test"
hf_revision = "benchmark_hf_revision_test"
hf_split = "benchmark_hf_split_test"
num_prompts = 500
seed = 42

[vllm]
model = "meta-llama/Llama-3.2-3B-Instruct"
seed = 42
dtype = "half"
kv-cache-dtype = "auto"
max-model-len = "4096"
max-num-batched-tokens = ""
max-num-seqs = 1024
tokenizer-mode = "auto"
enable-prefix-caching = ""
quantization = "aqlm"
enforce-eager = ""
enable-chunked-prefill = ""
pipeline-parallel-size = ""
tensor-parallel-size = ""
cpu-offload-gb = ""
gpu-memory-utilization = ""
device = "auto"
task = "auto"
tokenizer = ""
served-model-name = ""
skip-tokenizer-init = ""
revision = ""
code-revision = ""
tokenizer-revision = ""
trust-remote-code = ""
allowed-local-media-path = ""
download-dir = ""
load-format = "auto"
config-format = "auto"
guided-decoding-backend = "outlines"
logits-processor-pattern = ""
model-impl = "auto"
distributed-executor-backend = "ray"
max-parallel-loading-workers = ""
ray-workers-use-nsight = ""
block-size = "8"
disable-sliding-window = ""
num-lookahead-slots = ""
swap-space = ""
num-gpu-blocks-override = ""
max-logprobs = ""
disable-log-stats = ""
rope-scaling = ""
rope-theta = ""
hf-overrides = ""
max-seq-len-to-capture = ""
disable-custom-all-reduce = ""
tokenizer-pool-size = ""
tokenizer-pool-type = ""
tokenizer-pool-extra-config = ""
limit-mm-per-prompt = ""
mm-processor-kwargs = ""
disable-mm-preprocessor-cache = ""
enable-lora = ""
enable-lora-bias = ""
max-loras = ""
max-lora-rank = ""
lora-extra-vocab-size = ""
lora-dtype = "auto"
long-lora-scaling-factors = ""
max-cpu-loras = ""
fully-sharded-loras = ""
enable-prompt-adapter = ""
max-prompt-adapters = ""
max-prompt-adapter-token = ""
num-scheduler-steps = ""
multi-step-stream-outputs = ""
scheduler-delay-factor = ""
speculative-model = ""
speculative-model-quantization = "aqlm"
num-speculative-tokens = ""
speculative-disable-mqa-scorer = ""
speculative-draft-tensor-parallel-size = ""
speculative-max-model-len = ""
speculative-disable-by-batch-size = ""
ngram-prompt-lookup-max = ""
ngram-prompt-lookup-min = ""
spec-decoding-acceptance-method = "rejection-sampler"
typical-acceptance-sampler-posterior-threshold = ""
typical-acceptance-sampler-posterior-alpha = ""
disable-logprobs-during-spec-decoding = ""
model-loader-extra-config = ""
preemption-mode = ""
qlora-adapter-name-or-path = ""
otlp-traces-endpoint = ""
collect-detailed-traces = ""
disable-async-output-proc = ""
scheduling-policy = "fcfs"
override-neuron-config = ""
override-pooler-config = ""
compilation-config = ""
kv-transfer-config = ""
worker-cls = ""
generation-config = ""
override-generation-config = ""
enable-sleep-mode = ""
calculate-kv-scales = ""
additional-config = ""